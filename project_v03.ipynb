{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f32d42d-6429-4341-8304-7181ee28dbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from utils import *\n",
    "\n",
    "dataset = load_dataset('Cora')\n",
    "data = dataset[0] \n",
    "\n",
    "\n",
    "concat = edge_attributes('concat', dataset[0].edge_index, dataset[0].x)\n",
    "abs_ = edge_attributes('abs', dataset[0].edge_index, dataset[0].x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92f23a50-bb15-485d-8b16-157c7f765f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCN_ckpt = torch.load(\"checkpoints_v2/concat/best/model_GCN_epoch_974_loss_0.0332_concat.ckpt\")\n",
    "GAT_ckpt = torch.load(\"checkpoints_v2/concat/best/model_GAT_epoch_956_loss_0.0229_concat.ckpt\")\n",
    "GCN_V2_ckpt = torch.load(\"checkpoints_v2/abs/best/model_GCN_V2_epoch_760_loss_0.0082_abs.ckpt\")\n",
    "GAT_V2_ckpt = torch.load(\"checkpoints_v2/concat/best/model_GAT_V2_epoch_893_loss_0.0174_concat.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "464ef8fb-cdc1-4b6a-b9dc-266076d07dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "decay = 5e-4\n",
    "\n",
    "\n",
    "model_1 =  GCN(input_=dataset.num_features, hidden_channels=16, output_=dataset.num_classes)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model_1.parameters(), \n",
    "                             lr=learning_rate, \n",
    "                             weight_decay=decay)\n",
    "\n",
    "model_1.load_state_dict(GCN_ckpt['model_state_dict'])\n",
    "\n",
    "\n",
    "model_2 =  GAT(input_=dataset.num_features, hidden_channels=8, output_=dataset.num_classes)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model_2.parameters(), \n",
    "                             lr=learning_rate, \n",
    "                             weight_decay=decay)\n",
    "\n",
    "model_2.load_state_dict(GAT_ckpt['model_state_dict'])\n",
    "\n",
    "\n",
    "model_3 =  GCN_V2(input_=dataset.num_features, hidden_channels=16, edge_dim=abs_.shape[1], output_=dataset.num_classes)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model_3.parameters(), \n",
    "                             lr=learning_rate, \n",
    "                             weight_decay=decay)\n",
    "\n",
    "model_3.load_state_dict(GCN_V2_ckpt['model_state_dict'])\n",
    "\n",
    "model_4 =  GAT_V2(input_=dataset.num_features, hidden_channels=8, edge_dim=concat.shape[1], output_=dataset.num_classes)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model_4.parameters(), \n",
    "                             lr=learning_rate, \n",
    "                             weight_decay=decay)\n",
    "\n",
    "model_4.load_state_dict(GAT_V2_ckpt['model_state_dict'])\n",
    "\n",
    "models = {}\n",
    "\n",
    "models = {\n",
    "    \"GCN\": model_1,\n",
    "    \"GAT\": model_2,\n",
    "    \"GCN_V2\": model_3,\n",
    "    \"GAT_V2\": model_4,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be0566f-ed34-4c0b-82c2-dab483dabe96",
   "metadata": {},
   "source": [
    "# Logit Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90767ff5-629a-4894-b212-c98743c28482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy via simple averaging is: 0.801\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = []\n",
    "    for name, net in models.items():\n",
    "        if 'GCN_V2' in name:                 # needs edge attributes\n",
    "            out = net(dataset[0].x, dataset[0].edge_index, abs_)\n",
    "        elif 'GAT_V2' in name:\n",
    "            out = net(dataset[0].x, dataset[0].edge_index, concat)\n",
    "        else:\n",
    "            out = net(dataset[0].x, dataset[0].edge_index)\n",
    "        logits.append(out)               # shape: [num_nodes, num_classes]\n",
    "logits = torch.stack(logits)   \n",
    "\n",
    "avg_logits = logits.mean(dim=0)                      # [N, C]\n",
    "y_pred_avg = avg_logits.argmax(dim=1)    \n",
    "\n",
    "\n",
    "test_correct = y_pred_avg[dataset[0].test_mask] == dataset[0].y[dataset[0].test_mask]  \n",
    "test_acc = int(test_correct.sum()) / int(dataset[0].test_mask.sum())  \n",
    "\n",
    "\n",
    "print(f\"Testing Accuracy via simple averaging is: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a24a30e-963c-4701-ae92-63540db87d6a",
   "metadata": {},
   "source": [
    "# Weighted Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43f46885-cd5d-4d2e-8453-143834b4285f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy via weighted averaging is: 0.811\n"
     ]
    }
   ],
   "source": [
    "# Example: weights proportional to validation accuracy you measured earlier\n",
    "weights = torch.tensor([0.2, 0.5, 0.2, 0.3])  \n",
    "weights = weights / weights.sum()                   # normalise\n",
    "weighted_logits = (logits * weights.view(-1, 1, 1)).sum(dim=0)\n",
    "y_pred_wavg     = weighted_logits.argmax(dim=1)\n",
    "\n",
    "test_correct = y_pred_wavg[dataset[0].test_mask] == dataset[0].y[dataset[0].test_mask]  \n",
    "test_acc = int(test_correct.sum()) / int(dataset[0].test_mask.sum())  \n",
    "\n",
    "\n",
    "print(f\"Testing Accuracy via weighted averaging is: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3578d64e-b79d-4b03-a475-f99e035e601d",
   "metadata": {},
   "source": [
    "## GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "56c23f7d-9429-4cdd-a3ec-9d218e058857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best acc = 0.819 weights (np.float64(0.05), np.float64(0.55), np.float64(0.2), np.float64(0.2))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "\n",
    "M = logits.size(0) \n",
    "\n",
    "grid = np.arange(0, 1.01, 0.05)\n",
    "best, best_w = 0.0, None\n",
    "for w in itertools.product(grid, repeat=M):\n",
    "    if abs(sum(w)-1) > 1e-6:        # skip invalid tuples\n",
    "        continue\n",
    "    w_t = torch.tensor(w).view(-1,1,1)\n",
    "    comb = (logits * w_t).sum(dim=0)\n",
    "    pred = comb.argmax(dim = 1)\n",
    "    pred_correct = pred[dataset[0].test_mask] == dataset[0].y[dataset[0].test_mask]  \n",
    "    acc  = int(pred_correct.sum()) / int(dataset[0].test_mask.sum())\n",
    "    #print(f\"Accuracy {acc}\")\n",
    "    if acc > best:\n",
    "        best, best_w = acc, w\n",
    "print(\"best acc =\", best, \"weights\", best_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67462c9f-64bd-4875-921d-07b6a230f82c",
   "metadata": {},
   "source": [
    "# Non-linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425550a0-bc50-4d2b-a654-67a98fb66170",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3f01ebf2-12f6-4d1c-bd75-56601d79f1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy via non-linear comb. is: 0.796\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MetaStack(nn.Module):\n",
    "    def __init__(self, num_classes: int, num_models: int = 4, hidden: int = 32):\n",
    "        super().__init__()\n",
    "        self.num_models  = num_models                     # store for forward()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.fc1 = nn.Linear(num_models * num_classes, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, num_classes)\n",
    "\n",
    "    def forward(self, logit_stack: torch.Tensor):\n",
    "        # Accept either [M, N, C] or [N, M, C]\n",
    "        if logit_stack.size(0) == self.num_models:        # [M, N, C] ➜ transpose\n",
    "            logit_stack = logit_stack.permute(1, 0, 2)    # → [N, M, C]\n",
    "\n",
    "        x = logit_stack.reshape(logit_stack.size(0), -1)  # [N, M*C]\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)                                # [N, C]\n",
    "\n",
    "\n",
    "\n",
    "meta = MetaStack(num_models=len(models),\n",
    "                 num_classes=dataset.num_classes)\n",
    "\n",
    "opt_meta = torch.optim.Adam(meta.parameters(), lr=1e-4)\n",
    "ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1000):\n",
    "    meta.train()\n",
    "    opt_meta.zero_grad()\n",
    "    out = meta(logits[:, dataset[0].val_mask])                # pass logits from base models\n",
    "    loss = ce(out, dataset[0].y[dataset[0].val_mask])\n",
    "    loss.backward()\n",
    "    opt_meta.step()\n",
    "# -----------------------------------------------\n",
    "\n",
    "meta.eval()\n",
    "with torch.no_grad():\n",
    "    meta_logits = meta(logits)                     # all nodes\n",
    "y_pred_meta = meta_logits.argmax(dim=1)\n",
    "\n",
    "test_correct = y_pred_meta[dataset[0].test_mask] == dataset[0].y[dataset[0].test_mask]  \n",
    "test_acc = int(test_correct.sum()) / int(dataset[0].test_mask.sum())  \n",
    "\n",
    "\n",
    "print(f\"Testing Accuracy via non-linear comb. is: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f44e5a81-1590-41b2-a443-f850770a7217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean |weight| per model:\n",
      "GCN       0.2195\n",
      "GAT       0.2649\n",
      "GCN_V2    0.2333\n",
      "GAT_V2    0.1188\n"
     ]
    }
   ],
   "source": [
    "def extract_effective_weights(meta: MetaStack):\n",
    "    \"\"\"\n",
    "    Return:\n",
    "        W_eff  –  tensor [num_classes, num_models, num_classes]\n",
    "        diag   –  tensor [num_models, num_classes]  (each model's direct weight for its own class)\n",
    "        scalar –  tensor [num_models]               (mean magnitude per model, optional)\n",
    "    \"\"\"\n",
    "    C   = 7\n",
    "    M   = 4 #meta.num_models\n",
    "    W1  = meta.fc1.weight.detach()            # [hidden, M*C]\n",
    "    W2  = meta.fc2.weight.detach()            # [C, hidden]\n",
    "\n",
    "    # Fold the two linear layers:  [C, hidden] × [hidden, M*C] → [C, M*C]\n",
    "    W_eff = torch.matmul(W2, W1)              # [C_out, M*C_in]\n",
    "    W_eff = W_eff.view(C, M, C)               # [C_out, M, C_in]\n",
    "\n",
    "    # Direct contribution of model m to *its own* class c\n",
    "    diag = W_eff.diagonal(dim1=0, dim2=2).T   # [M, C]\n",
    "\n",
    "    # A single importance score per model (mean absolute value across classes)\n",
    "    scalar = diag.abs().mean(dim=1)\n",
    "\n",
    "    return W_eff, diag, scalar\n",
    "\n",
    "\n",
    "W_eff, per_class, per_model = extract_effective_weights(meta)\n",
    "\n",
    "print(\"Mean |weight| per model:\")\n",
    "for name, score in zip(models.keys(), per_model):\n",
    "    print(f\"{name:<8s}  {score.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2761f38b-ddcb-400d-80ca-2c6c3608e77e",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "792a5c09-6315-4a85-a9da-dab8cf777421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy via non-linear comb. is: 0.793\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MetaLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    One-layer meta-learner (a.k.a. stacking logistic regression).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_models : int\n",
    "        Number of base models in the ensemble.\n",
    "    num_classes : int\n",
    "        Number of target classes.\n",
    "    bias : bool, default=True\n",
    "        Whether to include a bias term.\n",
    "    tie_classes : bool, default=False\n",
    "        • False  →  independent weight for every (model, class) pair\n",
    "                    (weight matrix shape = [num_classes , num_models * num_classes])\n",
    "        • True   →  ONE weight per model, shared across classes\n",
    "                    (weight vector shape = [num_models])\n",
    "    \"\"\"\n",
    "    def __init__(self, num_models: int, num_classes: int,\n",
    "                 bias: bool = True, tie_classes: bool = False):\n",
    "        super().__init__()\n",
    "        self.num_models  = num_models\n",
    "        self.num_classes = num_classes\n",
    "        self.tie         = tie_classes\n",
    "\n",
    "        if tie_classes:\n",
    "            # weight: [num_models]   → broadcast over classes inside `forward`\n",
    "            self.log_w = nn.Parameter(torch.zeros(num_models))  # unconstrained\n",
    "            self.bias   = nn.Parameter(torch.zeros(num_classes)) if bias else None\n",
    "        else:\n",
    "            # classic logistic-regression weight matrix\n",
    "            self.fc = nn.Linear(num_models * num_classes, num_classes, bias=bias)\n",
    "\n",
    "    def forward(self, logit_stack: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Accepts `logit_stack` in either shape:\n",
    "            • [M, N, C]   (M=models first)\n",
    "            • [N, M, C]   (N=nodes/samples first)\n",
    "        Returns:\n",
    "            logits_out : [N, C]\n",
    "        \"\"\"\n",
    "        if logit_stack.size(0) == self.num_models:          # [M, N, C] → [N, M, C]\n",
    "            logit_stack = logit_stack.permute(1, 0, 2)\n",
    "\n",
    "        if self.tie:\n",
    "            # softmax normalises weights so they sum to 1 and stay positive (optional)\n",
    "            w = torch.softmax(self.log_w, dim=0)            # [M]\n",
    "            out = (logit_stack * w.view(1, -1, 1)).sum(dim=1)  # [N, C]\n",
    "            if self.bias is not None:\n",
    "                out = out + self.bias                       # broadcast [C]\n",
    "            return out\n",
    "        else:\n",
    "            x = logit_stack.reshape(logit_stack.size(0), -1)  # [N, M*C]\n",
    "            return self.fc(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_models  = logits.size(0)        # 4\n",
    "num_classes = logits.size(2)        # 7 for Cora\n",
    "\n",
    "meta = MetaLinear(num_models, num_classes, bias=True,\n",
    "                  tie_classes=False    # set True to get ONE weight per model\n",
    "                 )\n",
    "\n",
    "opt = torch.optim.Adam(meta.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1000):\n",
    "    meta.train();  opt.zero_grad()\n",
    "    out = meta(logits[:, dataset[0].val_mask])                # pass logits from base models\n",
    "    loss = ce(out, dataset[0].y[dataset[0].val_mask])\n",
    "    loss.backward();  opt.step()\n",
    "\n",
    "\n",
    "meta.eval()\n",
    "with torch.no_grad():\n",
    "    meta_logits = meta(logits)                     # all nodes\n",
    "y_pred_meta = meta_logits.argmax(dim=1)\n",
    "\n",
    "\n",
    "test_correct = y_pred_meta[dataset[0].test_mask] == dataset[0].y[dataset[0].test_mask]  \n",
    "test_acc = int(test_correct.sum()) / int(dataset[0].test_mask.sum())  \n",
    "\n",
    "\n",
    "print(f\"Testing Accuracy via non-linear comb. is: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8868019-b448-49c8-872a-35593dea0e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model-class weight matrix:\n",
      " [[ 0.24713314  0.10607173  0.09416505  0.21312167]\n",
      " [ 0.29427725  0.24221335  0.06410835  0.2502378 ]\n",
      " [ 0.16815935  0.08430109  0.02704762  0.07807249]\n",
      " [ 0.05438573  0.05874786  0.01834708  0.03069128]\n",
      " [ 0.19682418  0.47851738  0.17404401 -0.04576796]\n",
      " [ 0.07480611  0.3945138   0.09425414 -0.04250543]\n",
      " [ 0.21857044  0.29283202  0.14897801  0.08563743]]\n"
     ]
    }
   ],
   "source": [
    "if meta.tie:\n",
    "    print(\"Per-model weights:\", torch.softmax(meta.log_w, dim=0).cpu().numpy())\n",
    "else:\n",
    "    W = meta.fc.weight.detach().cpu()                 # [C, M*C]\n",
    "    W = W.view(num_classes, num_models, num_classes)  # [C_out, M, C_in]\n",
    "    diag = W.diagonal(dim1=0, dim2=2).T              # [M, C]  (model × class)\n",
    "    print(\"Model-class weight matrix:\\n\", diag.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0162372d-800d-47e0-8fe5-32511beb6988",
   "metadata": {},
   "source": [
    "## Meta MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "79306d55-985a-4186-8f37-f4f6de2737da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy via non-linear comb. is: 0.801\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "class MetaMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    3-layer MLP with BatchNorm, GELU and Dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_models: int, num_classes: int,\n",
    "                 width: int = 64, p_drop: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.num_models  = num_models\n",
    "        self.num_classes = num_classes\n",
    "        d_in  = num_models * num_classes              # M * C\n",
    "        d_mid = width\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_in,  d_mid),\n",
    "            nn.BatchNorm1d(d_mid),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p_drop),\n",
    "\n",
    "            nn.Linear(d_mid, d_mid),\n",
    "            nn.BatchNorm1d(d_mid),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p_drop),\n",
    "\n",
    "            nn.Linear(d_mid, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, logit_stack: torch.Tensor) -> torch.Tensor:\n",
    "        if logit_stack.size(0) == self.num_models:            # [M, N, C] → [N, M, C]\n",
    "            logit_stack = logit_stack.permute(1, 0, 2)\n",
    "\n",
    "        x = logit_stack.reshape(logit_stack.size(0), -1)      # [N, M*C]\n",
    "        return self.net(x)                                    # [N, C]\n",
    "\n",
    "\n",
    "\n",
    "meta = MetaMLP(num_models=len(models),\n",
    "                 num_classes=dataset.num_classes, width=24, p_drop=0.3)\n",
    "\n",
    "opt_meta = torch.optim.AdamW(meta.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1000):\n",
    "    meta.train()\n",
    "    opt_meta.zero_grad()\n",
    "    out = meta(logits[:, dataset[0].val_mask])                # pass logits from base models\n",
    "    loss = ce(out, dataset[0].y[dataset[0].val_mask])\n",
    "    loss.backward()\n",
    "    opt_meta.step()\n",
    "# -----------------------------------------------\n",
    "\n",
    "meta.eval()\n",
    "with torch.no_grad():\n",
    "    meta_logits = meta(logits)                     # all nodes\n",
    "y_pred_meta = meta_logits.argmax(dim=1)\n",
    "\n",
    "test_correct = y_pred_meta[dataset[0].test_mask] == dataset[0].y[dataset[0].test_mask]  \n",
    "test_acc = int(test_correct.sum()) / int(dataset[0].test_mask.sum())  \n",
    "\n",
    "\n",
    "print(f\"Testing Accuracy via non-linear comb. is: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d507c9-601f-4013-86cb-ccbe8876f02a",
   "metadata": {},
   "source": [
    "## Meta Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fcdd1fb3-8741-4f6e-b78c-84e2ce04e2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy via non-linear comb. is: 0.805\n"
     ]
    }
   ],
   "source": [
    "class MetaConv1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Learns interactions with depth-wise + point-wise 1-D convs across models.\n",
    "    Treats the logits of each class as a separate 'channel'.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_models: int, num_classes: int,\n",
    "                 hidden: int = 32, kernel_size: int = 3):\n",
    "        \"\"\"\n",
    "        kernel_size should be odd (3 or 5). If num_models < kernel_size, it is clipped.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_models  = num_models\n",
    "        self.num_classes = num_classes\n",
    "        ks = min(kernel_size, num_models)\n",
    "        padding = ks // 2                                    # keep length\n",
    "\n",
    "        # Depth-wise conv (per class)\n",
    "        self.depthwise = nn.Conv1d(in_channels=num_classes,\n",
    "                                   out_channels=num_classes,\n",
    "                                   kernel_size=ks,\n",
    "                                   groups=num_classes,        # depth-wise\n",
    "                                   padding=padding, bias=False)\n",
    "\n",
    "        # Point-wise conv mixes classes & models\n",
    "        self.pointwise = nn.Conv1d(in_channels=num_classes,\n",
    "                                   out_channels=hidden,\n",
    "                                   kernel_size=1)\n",
    "\n",
    "        self.act  = nn.GELU()\n",
    "        self.out  = nn.Linear(hidden, num_classes)\n",
    "\n",
    "    def forward(self, logit_stack: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        logit_stack: [M, N, C] or [N, M, C]\n",
    "        \"\"\"\n",
    "        if logit_stack.size(0) == self.num_models:            # [M, N, C] → [N, M, C]\n",
    "            logit_stack = logit_stack.permute(1, 0, 2)\n",
    "\n",
    "        # reshape for Conv1d:  [N, M, C] → [N, C, M]\n",
    "        x = logit_stack.permute(0, 2, 1)\n",
    "\n",
    "        x = self.depthwise(x)                                 # depth-wise conv\n",
    "        x = self.act(self.pointwise(x))                      # point-wise conv\n",
    "        x = torch.mean(x, dim=2)                             # global-avg over model axis  → [N, hidden]\n",
    "        return self.out(x)                                   # [N, C]\n",
    "\n",
    "\n",
    "\n",
    "meta = MetaConv1D(num_models=len(models),\n",
    "                 num_classes=dataset.num_classes, hidden=32, kernel_size=3)\n",
    "\n",
    "opt_meta = torch.optim.AdamW(meta.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "ce = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1000):\n",
    "    meta.train()\n",
    "    opt_meta.zero_grad()\n",
    "    out = meta(logits[:, dataset[0].val_mask])                # pass logits from base models\n",
    "    loss = ce(out, dataset[0].y[dataset[0].val_mask])\n",
    "    loss.backward()\n",
    "    opt_meta.step()\n",
    "# -----------------------------------------------\n",
    "\n",
    "meta.eval()\n",
    "with torch.no_grad():\n",
    "    meta_logits = meta(logits)                     # all nodes\n",
    "y_pred_meta = meta_logits.argmax(dim=1)\n",
    "\n",
    "test_correct = y_pred_meta[dataset[0].test_mask] == dataset[0].y[dataset[0].test_mask]  \n",
    "test_acc = int(test_correct.sum()) / int(dataset[0].test_mask.sum())  \n",
    "\n",
    "\n",
    "print(f\"Testing Accuracy via non-linear comb. is: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb87b6d9-d48c-4ff1-b3c2-6e91c40a8299",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
